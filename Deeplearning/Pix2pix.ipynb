{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f699e-dc5e-4d1d-888a-33341068c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3278462-30f4-4538-963d-2f44f260c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz -O facades.tar.gz\n",
    "!tar -zxvf facades.tar.gz -C ./\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55234661-9e57-443f-a1a0-3d170fba2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"학습 데이터셋 A와 B의 개수:\", len(next(os.walk('./facades/train/'))[2]))\n",
    "print(\"평가 데이터셋 A와 B의 개수:\", len(next(os.walk('./facades/val/'))[2]))\n",
    "print(\"테스트 데이터셋 A와 B의 개수:\", len(next(os.walk('./facades/test/'))[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923843c-758f-4ab6-9eb5-22ba70e7d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 쌍의 이미지 출력(왼쪽은 정답 이미지, 오른쪽은 조건 이미지)\n",
    "image = Image.open('./facades/train/1.jpg')\n",
    "print(\"이미지 크기:\", image.size)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6e65e1-5fa1-49d7-807d-e37023c2175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, mode=\"train\"):\n",
    "        self.transform = transforms_\n",
    "\n",
    "        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.jpg\"))\n",
    "        # 데이터의 개수가 적기 때문에 테스트 데이터를 학습 시기에 사용\n",
    "        if mode == \"train\":\n",
    "            self.files.extend(sorted(glob.glob(os.path.join(root, \"test\") + \"/*.jpg\")))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        w, h = img.size\n",
    "        img_A = img.crop((0, 0, w / 2, h)) # 이미지의 왼쪽 절반\n",
    "        img_B = img.crop((w / 2, 0, w, h)) # 이미지의 오른쪽 절반\n",
    "\n",
    "        # 데이터 증진(data augmentation)을 위한 좌우 반전(horizontal flip)\n",
    "        if np.random.random() < 0.5:\n",
    "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
    "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
    "\n",
    "        img_A = self.transform(img_A)\n",
    "        img_B = self.transform(img_B)\n",
    "\n",
    "        return {\"A\": img_A, \"B\": img_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864515d6-218d-47f7-b83d-35e19d531439",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ = transforms.Compose([\n",
    "    transforms.Resize((256, 256), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(\"facades\", transforms_=transforms_)\n",
    "val_dataset = ImageDataset(\"facades\", transforms_=transforms_)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=True, num_workers=4)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf775db-9fcd-4014-999e-30d6f8fdd896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net 아키텍처의 다운 샘플링(Down Sampling) 모듈\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        # 너비와 높이가 2배씩 감소\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# U-Net 아키텍처의 업 샘플링(Up Sampling) 모듈: Skip Connection 사용\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        # 너비와 높이가 2배씩 증가\n",
    "        layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1) # 채널 레벨에서 합치기(concatenation)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a9a75b-307f-4031-a789-e842c17eb511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net 생성자(Generator) 아키텍처\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False) # 출력: [64 X 128 X 128]\n",
    "        self.down2 = UNetDown(64, 128) # 출력: [128 X 64 X 64]\n",
    "        self.down3 = UNetDown(128, 256) # 출력: [256 X 32 X 32]\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5) # 출력: [512 X 16 X 16]\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5) # 출력: [512 X 8 X 8]\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5) # 출력: [512 X 4 X 4]\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5) # 출력: [512 X 2 X 2]\n",
    "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5) # 출력: [512 X 1 X 1]\n",
    "\n",
    "        # Skip Connection 사용(출력 채널의 크기 X 2 == 다음 입력 채널의 크기)\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5) # 출력: [1024 X 2 X 2]\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5) # 출력: [1024 X 4 X 4]\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5) # 출력: [1024 X 8 X 8]\n",
    "        self.up4 = UNetUp(1024, 512, dropout=0.5) # 출력: [1024 X 16 X 16]\n",
    "        self.up5 = UNetUp(1024, 256) # 출력: [512 X 32 X 32]\n",
    "        self.up6 = UNetUp(512, 128) # 출력: [256 X 64 X 64]\n",
    "        self.up7 = UNetUp(256, 64) # 출력: [128 X 128 X 128]\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2), # 출력: [128 X 256 X 256]\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(128, out_channels, kernel_size=4, padding=1), # 출력: [3 X 256 X 256]\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 인코더부터 디코더까지 순전파하는 U-Net 생성자(Generator)\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "\n",
    "        return self.final(u7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00b5f9-b6ee-4fd8-946d-1248dcd9f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net 판별자(Discriminator) 아키텍처\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_channels, out_channels, normalization=True):\n",
    "            # 너비와 높이가 2배씩 감소\n",
    "            layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # 두 개의 이미지(실제/변환된 이미지, 조건 이미지)를 입력 받으므로 입력 채널의 크기는 2배\n",
    "            *discriminator_block(in_channels * 2, 64, normalization=False), # 출력: [64 X 128 X 128]\n",
    "            *discriminator_block(64, 128), # 출력: [128 X 64 X 64]\n",
    "            *discriminator_block(128, 256), # 출력: [256 X 32 X 32]\n",
    "            *discriminator_block(256, 512), # 출력: [512 X 16 X 16]\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1, bias=False) # 출력: [1 X 16 X 16]\n",
    "        )\n",
    "\n",
    "    # img_A: 실제/변환된 이미지, img_B: 조건(condition)\n",
    "    def forward(self, img_A, img_B):\n",
    "        # 이미지 두 개를 채널 레벨에서 연결하여(concatenate) 입력 데이터 생성\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d7855-372c-4e5e-8cc8-be339d6ae745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "# 생성자(generator)와 판별자(discriminator) 초기화\n",
    "generator = GeneratorUNet()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.cuda()\n",
    "discriminator.cuda()\n",
    "\n",
    "# 가중치(weights) 초기화\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# 손실 함수(loss function)\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "\n",
    "criterion_GAN.cuda()\n",
    "criterion_pixelwise.cuda()\n",
    "\n",
    "# 학습률(learning rate) 설정\n",
    "lr = 0.0002\n",
    "\n",
    "# 생성자와 판별자를 위한 최적화 함수\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b6762-8800-4eb5-9cf7-ff92626d5ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 200 # 학습의 횟수(epoch) 설정\n",
    "sample_interval = 200 # 몇 번의 배치(batch)마다 결과를 출력할 것인지 설정\n",
    "\n",
    "# 변환된 이미지와 정답 이미지 사이의 L1 픽셀 단위(pixel-wise) 손실 가중치(weight) 파라미터\n",
    "lambda_pixel = 100\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        # 모델의 입력(input) 데이터 불러오기\n",
    "        real_A = batch[\"B\"].cuda()\n",
    "        real_B = batch[\"A\"].cuda()\n",
    "\n",
    "        # 진짜(real) 이미지와 가짜(fake) 이미지에 대한 정답 레이블 생성 (너바와 높이를 16씩 나눈 크기)\n",
    "        real = torch.cuda.FloatTensor(real_A.size(0), 1, 16, 16).fill_(1.0) # 진짜(real): 1\n",
    "        fake = torch.cuda.FloatTensor(real_A.size(0), 1, 16, 16).fill_(0.0) # 가짜(fake): 0\n",
    "\n",
    "        \"\"\" 생성자(generator)를 학습합니다. \"\"\"\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # 이미지 생성\n",
    "        fake_B = generator(real_A)\n",
    "\n",
    "        # 생성자(generator)의 손실(loss) 값 계산\n",
    "        loss_GAN = criterion_GAN(discriminator(fake_B, real_A), real)\n",
    "\n",
    "        # 픽셀 단위(pixel-wise) L1 손실 값 계산\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B) \n",
    "        \n",
    "        # 최종적인 손실(loss)\n",
    "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "        # 생성자(generator) 업데이트\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        \"\"\" 판별자(discriminator)를 학습합니다. \"\"\"\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # 판별자(discriminator)의 손실(loss) 값 계산\n",
    "        loss_real = criterion_GAN(discriminator(real_B, real_A), real) # 조건(condition): real_A\n",
    "        loss_fake = criterion_GAN(discriminator(fake_B.detach(), real_A), fake)\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "\n",
    "        # 판별자(discriminator) 업데이트\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        done = epoch * len(train_dataloader) + i\n",
    "        if done % sample_interval == 0:\n",
    "            imgs = next(iter(val_dataloader)) # 10개의 이미지를 추출해 생성\n",
    "            real_A = imgs[\"B\"].cuda()\n",
    "            real_B = imgs[\"A\"].cuda()\n",
    "            fake_B = generator(real_A)\n",
    "            # real_A: 조건(condition), fake_B: 변환된 이미지(translated image), real_B: 정답 이미지\n",
    "            img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "            save_image(img_sample, f\"{done}.png\", nrow=5, normalize=True)\n",
    "\n",
    "    # 하나의 epoch이 끝날 때마다 로그(log) 출력\n",
    "    print(f\"[Epoch {epoch}/{n_epochs}] [D loss: {loss_D.item():.6f}] [G pixel loss: {loss_pixel.item():.6f}, adv loss: {loss_GAN.item()}] [Elapsed time: {time.time() - start_time:.2f}s]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3663a02-a74e-4295-b5ed-0673a964af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image('10000.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4bdb1e-28d6-42c1-9454-85a954737e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 모델 파라미터 저장\n",
    "torch.save(generator.state_dict(), \"Pix2Pix_Generator_for_Facades.pt\")\n",
    "torch.save(discriminator.state_dict(), \"Pix2Pix_Discriminator_for_Facades.pt\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f2fecf-52e9-4e0f-be85-8d9cac9bed82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mulan",
   "language": "python",
   "name": "mulan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
